{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44eb9187",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "Combining the results of many classifiers (even each classifier is not strong) will produce a result that is better than the strongest classifier of them\n",
    "\n",
    "## Ensemble Algorithms:\n",
    "### Voting:\n",
    "- Hard Voting:\n",
    "    - Get the **most frequent** predicted class **without** considering probabilities\n",
    "- Soft Voting (better):\n",
    "    - Get the **most frequent** predicted class but **consider probabilities** (assign bigger weight to **high confidant votes**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2593d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "97a37793",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 25\n",
    "decision_tree = DecisionTreeClassifier(max_depth=3, random_state=random_state)\n",
    "svc = SVC(random_state=random_state)\n",
    "logestic_reg = LogisticRegression(random_state=random_state,max_iter=2000)\n",
    "\n",
    "# combine all the classifier and \n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"svc\", svc),\n",
    "        (\"log_reg\", logestic_reg),\n",
    "        (\"decision_tree\", decision_tree)\n",
    "    ],\n",
    "    \n",
    "    voting=\"hard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f8ae959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "dataset = load_breast_cancer()\n",
    "# dataset['feature_names'], dataset['target_names']\n",
    "X, y = dataset['data'], dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1d713d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f519927d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((426, 30), (143, 30), (426,), (143,))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0f68ee05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier :  0.965034965034965\n",
      "SVC :  0.9300699300699301\n",
      "LogisticRegression :  0.958041958041958\n",
      "VotingClassifier :  0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# train and test each classifier vs votin classifier\n",
    "def test_voting():\n",
    "    for clf in (decision_tree, svc, logestic_reg, voting_clf):\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        score = accuracy_score(predictions, y_test)\n",
    "        print(clf.__class__.__name__, \": \", score)\n",
    "\n",
    "test_voting()\n",
    "# Voting classifier is better than the best of them !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d2d1f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to make probability=True so that SVM change its implementation to predict proability\n",
    "svc = SVC(random_state=random_state, probability=True)\n",
    "\n",
    "# all estimators in the voting should have predict_prob method to make soft vorting\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"svc\", svc),\n",
    "        (\"log_reg\", logestic_reg),\n",
    "        (\"decision_tree\", decision_tree)\n",
    "    ],\n",
    "    \n",
    "    voting=\"soft\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "12c82716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier :  0.965034965034965\n",
      "SVC :  0.9300699300699301\n",
      "LogisticRegression :  0.958041958041958\n",
      "VotingClassifier :  0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "test_voting()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16645695",
   "metadata": {},
   "source": [
    "# Bagging and Pasting\n",
    "Instead of using many classifiers, we use the same classifier on different training data samples. <br>\n",
    "\n",
    "- Bagging: sampling of training data is **with replacempent** (some instances are shared between classifiers)\n",
    "- Pasting: sampling of training data is **without** replacempent\n",
    "\n",
    "both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor\n",
    "\n",
    "![bagging](images/ensemble.png)\n",
    "\n",
    "\n",
    "- The aggregation function is typically the statistical mode (i.e., the **most frequent** prediction, just like a\n",
    "hard voting classifier) **for classification**, or the **average** for **regression**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3c0084b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# max_samples: each trained on 100 training instances randomly sampled from the training set with replacement\n",
    "# if you want to use pasting instead, just set bootstrap=False)\n",
    "# The n_jobs parameter tells Scikit-Learn the number of CPU cores to use for training and predictions (–1 tells Scikit-Learn to use all available cores)\n",
    "# The BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can estimate class proba‐ bilities (i.e., if it has a predict_proba() method),\n",
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), \n",
    "                            n_estimators=500, \n",
    "                            max_samples=100, \n",
    "                            bootstrap=True,\n",
    "                            n_jobs=-1,\n",
    "                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e776782a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f0e63032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "predictions = bagging.predict(X_test)\n",
    "print(accuracy_score(predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9a837cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "# increasing max samples lead to more generalization\n",
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), \n",
    "                            n_estimators=500, max_samples=200, bootstrap=True, n_jobs=-1)\n",
    "bagging.fit(X_train, y_train)\n",
    "predictions = bagging.predict(X_test)\n",
    "print(accuracy_score(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4593fa70",
   "metadata": {},
   "source": [
    "# Random Patches and Random Subspaces\n",
    "The BaggingClassifier class supports sampling the features as well. This is controlled by two hyperparameters: **max_features and bootstrap_features**, They work the same way as **max_samples and bootstrap**, but for **feature sampling** instead of **instance sampling**. Thus, each predictor will be trained on a **random subset of the input features**.\n",
    "\n",
    "Useful when dealing with high dimension data (such as images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f95b9c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972027972027972\n"
     ]
    }
   ],
   "source": [
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), \n",
    "                            n_estimators=500, max_samples=200, bootstrap=True, n_jobs=-1,\n",
    "                            max_features=10, bootstrap_features=True)\n",
    "bagging.fit(X_train, y_train)\n",
    "predictions = bagging.predict(X_test)\n",
    "print(accuracy_score(predictions, y_test))\n",
    "# better !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1f2fb",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "Random Forests is a bagging of decision trees with max_samples = size of the training set\n",
    "\n",
    "- a RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier (to control how trees are grown), plus all the hyperparameters of a BaggingClassifier to control the ensemble itself.\n",
    "\n",
    "- The Random Forest algorithm instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features (better result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fff92cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100,\n",
    "                                       n_jobs=-1,\n",
    "                                       max_leaf_nodes=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e936e389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=16, n_jobs=-1)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4aaaec97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9790209790209791"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = random_forest.predict(X_test)\n",
    "accuracy_score(predictions, y_test) # Best result till now !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217680d4",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "- Random Forests used to know the importance of each feature in the data\n",
    "\n",
    "- Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest), and all importance of all features sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f21940f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the importance of each feature in iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_forest = RandomForestClassifier(n_estimators=500, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c4a75f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest.fit(iris[\"data\"], iris[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "83205bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " array([0.09582688, 0.025055  , 0.42632722, 0.4527909 ]))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris[\"feature_names\"], rnd_forest.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e899e21",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "training predictors sequentially to improve them (using the weights of the prev predictor as initial)\n",
    "## AdaBoost\n",
    "One way for a new predictor to correct its predecessor(prev) is to pay a bit more attention\n",
    "to the training instances that the predecessor underfitted. This results in new predic‐\n",
    "tors focusing more and more on the hard cases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71f07d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
